{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, dataset_name=\"\"):\n",
    "    # Calculate confusion matrix components\n",
    "    TP = np.sum((y_pred == 'spam') & (y_true == 'spam'))\n",
    "    FP = np.sum((y_pred == 'spam') & (y_true == 'ham'))\n",
    "    TN = np.sum((y_pred == 'ham') & (y_true == 'ham'))\n",
    "    FN = np.sum((y_pred == 'ham') & (y_true == 'spam'))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Metrics for {dataset_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'confusion_matrix': {\n",
    "            'TP': TP,\n",
    "            'FP': FP,\n",
    "            'TN': TN,\n",
    "            'FN': FN\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tập dữ liệu Enron-Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/train.csv')\n",
    "df_val = pd.read_csv('Data/val.csv')\n",
    "\n",
    "df_train.drop(columns=['Message ID', 'Unnamed: 0', 'split'], inplace=True)\n",
    "df_val.drop(columns=['Message ID', 'Unnamed: 0', 'split'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['Spam/Ham'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {len(df_train)}\")\n",
    "print(f\"Number of rows: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Phân tích dữ liệu thiếu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values in training set:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\nMissing values in validation set:\")\n",
    "print(df_val.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Xử lý dữ liệu thiếu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Subject'] = df_train['Subject'].fillna('')\n",
    "df_train['Message'] = df_train['Message'].fillna('')\n",
    "df_val['Subject'] = df_val['Subject'].fillna('')\n",
    "df_val['Message'] = df_val['Message'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Kết hợp Subject và Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['Subject'] + ' ' + df_train['Message']\n",
    "df_val['text'] = df_val['Subject'] + ' ' + df_val['Message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Chuyển dữ liệu về dạng vector BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = bow.fit_transform(df_train['text'])\n",
    "X_val = bow.transform(df_val['text'])\n",
    "\n",
    "y_train = df_train['Spam/Ham']\n",
    "y_val = df_val['Spam/Ham']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiện thực mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  \n",
    "        self.class_priors = None\n",
    "        self.feature_probs = None\n",
    "        self.classes = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"Input array X is empty\")\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "            \n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        self.class_priors = np.zeros(n_classes)\n",
    "        for i, c in enumerate(self.classes):\n",
    "            self.class_priors[i] = np.sum(y == c) / n_samples\n",
    "            \n",
    "\n",
    "        self.feature_probs = np.zeros((n_classes, n_features))\n",
    "        for i, c in enumerate(self.classes):\n",
    "            class_indices = np.where(y == c)[0]\n",
    "            \n",
    "            feature_counts = np.zeros(n_features) + self.alpha\n",
    "            batch_size = 1000\n",
    "            \n",
    "            for start_idx in range(0, len(class_indices), batch_size):\n",
    "                end_idx = min(start_idx + batch_size, len(class_indices))\n",
    "                batch_indices = class_indices[start_idx:end_idx]\n",
    "                \n",
    "                if isinstance(X, np.ndarray):\n",
    "                    batch_sum = X[batch_indices].sum(axis=0)\n",
    "                else:\n",
    "                    batch_sum = X[batch_indices].toarray().sum(axis=0)\n",
    "                    \n",
    "                feature_counts += batch_sum\n",
    "                \n",
    "            total_counts = feature_counts.sum()\n",
    "            self.feature_probs[i] = feature_counts / total_counts\n",
    "            \n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "            \n",
    "        predictions = []\n",
    "        batch_size = 1000\n",
    "        \n",
    "        for start_idx in range(0, X.shape[0], batch_size):\n",
    "            end_idx = min(start_idx + batch_size, X.shape[0])\n",
    "            if isinstance(X, np.ndarray):\n",
    "                batch = X[start_idx:end_idx]\n",
    "            else:\n",
    "                batch = X[start_idx:end_idx].toarray()\n",
    "                \n",
    "            batch_predictions = np.array([self._predict_single(x) for x in batch])\n",
    "            predictions.extend(batch_predictions)\n",
    "            \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        log_probs = np.log(self.class_priors)\n",
    "        \n",
    "        for i in range(len(self.classes)):\n",
    "            present_features = x > 0\n",
    "            if np.any(present_features):\n",
    "                log_probs[i] += np.sum(np.log(self.feature_probs[i][present_features]) * x[present_features])\n",
    "        \n",
    "        return self.classes[np.argmax(log_probs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(y_train, nb.predict(X_train), \"Training\")\n",
    "print(\"--------------------------------\")\n",
    "calculate_metrics(y_val, nb.predict(X_val), \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiện thực mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, batch_size=1000, tol=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []  \n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        z = np.clip(z, -250, 250)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _get_batches(self, X, y, batch_size):\n",
    "        n_samples = X.shape[0]\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            end_idx = min(i + batch_size, n_samples)\n",
    "            if sparse.issparse(X):\n",
    "                yield X[i:end_idx], y[i:end_idx]\n",
    "            else:\n",
    "                yield X[i:end_idx], y[i:end_idx]\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if isinstance(y.iloc[0], str):\n",
    "            y = (y == 'spam').astype(int)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            if isinstance(y_val.iloc[0], str):\n",
    "                y_val = (y_val == 'spam').astype(int)\n",
    "            if hasattr(y_val, 'values'):\n",
    "                y_val = y_val.values\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        prev_cost = float('inf')\n",
    "        self.loss_history = []  \n",
    "        self.val_loss_history = [] \n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            total_cost = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            indices = np.random.permutation(n_samples)\n",
    "            if sparse.issparse(X):\n",
    "                X_shuffled = X[indices]\n",
    "            else:\n",
    "                X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for X_batch, y_batch in self._get_batches(X_shuffled, y_shuffled, self.batch_size):\n",
    "                batch_size = X_batch.shape[0]\n",
    "                \n",
    "                if sparse.issparse(X_batch):\n",
    "                    z = X_batch.dot(self.weights) + self.bias\n",
    "                else:\n",
    "                    z = np.dot(X_batch, self.weights) + self.bias\n",
    "                \n",
    "                predictions = self._sigmoid(z)\n",
    "                \n",
    "                epsilon = 1e-15\n",
    "                predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "                batch_cost = -np.mean(y_batch * np.log(predictions) + \n",
    "                                    (1 - y_batch) * np.log(1 - predictions))\n",
    "                total_cost += batch_cost * batch_size\n",
    "                n_batches += batch_size\n",
    "                \n",
    "                dz = predictions - y_batch\n",
    "                if sparse.issparse(X_batch):\n",
    "                    dw = X_batch.T.dot(dz) / batch_size\n",
    "                else:\n",
    "                    dw = np.dot(X_batch.T, dz) / batch_size\n",
    "                db = np.mean(dz)\n",
    "                \n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "            \n",
    "            avg_cost = total_cost / n_batches\n",
    "            self.loss_history.append(avg_cost)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_loss = self._calculate_loss(X_val, y_val)\n",
    "                self.val_loss_history.append(val_loss)\n",
    "            \n",
    "            if abs(prev_cost - avg_cost) < self.tol:\n",
    "                print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "                \n",
    "            prev_cost = avg_cost\n",
    "            \n",
    "            if (iteration + 1) % 100 == 0:\n",
    "                val_info = f\", Val Loss: {self.val_loss_history[-1]:.6f}\" if X_val is not None else \"\"\n",
    "                print(f\"Iteration {iteration + 1}, Train Loss: {avg_cost:.6f}{val_info}\")\n",
    "    \n",
    "    def _calculate_loss(self, X, y):\n",
    "        if sparse.issparse(X):\n",
    "            z = X.dot(self.weights) + self.bias\n",
    "        else:\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        predictions = self._sigmoid(z)\n",
    "        epsilon = 1e-15\n",
    "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "        \n",
    "        return -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        probabilities = np.zeros(n_samples)\n",
    "        \n",
    "        start_idx = 0\n",
    "        for X_batch, _ in self._get_batches(X, np.zeros(n_samples), self.batch_size):\n",
    "            batch_size = X_batch.shape[0]\n",
    "            \n",
    "            if sparse.issparse(X_batch):\n",
    "                z = X_batch.dot(self.weights) + self.bias\n",
    "            else:\n",
    "                z = np.dot(X_batch, self.weights) + self.bias\n",
    "            \n",
    "            batch_proba = self._sigmoid(z)\n",
    "            probabilities[start_idx:start_idx + batch_size] = batch_proba\n",
    "            start_idx += batch_size\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "    \n",
    "    def plot_loss(self, figsize=(12, 5)):\n",
    "        if not self.loss_history:\n",
    "            print(\"No loss history available. Train the model first.\")\n",
    "            return\n",
    "        \n",
    "        if len(self.val_loss_history) > 0:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "            \n",
    "            ax1.plot(self.loss_history, label='Training Loss', color='blue', linewidth=2)\n",
    "            ax1.plot(self.val_loss_history, label='Validation Loss', color='red', linewidth=2)\n",
    "            ax1.set_xlabel('Iteration')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training vs Validation Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            ax2.semilogy(self.loss_history, label='Training Loss', color='blue', linewidth=2)\n",
    "            ax2.semilogy(self.val_loss_history, label='Validation Loss', color='red', linewidth=2)\n",
    "            ax2.set_xlabel('Iteration')\n",
    "            ax2.set_ylabel('Loss (log scale)')\n",
    "            ax2.set_title('Loss Convergence (Log Scale)')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "            \n",
    "            ax1.plot(self.loss_history, color='blue', linewidth=2)\n",
    "            ax1.set_xlabel('Iteration')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training Loss')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            ax2.semilogy(self.loss_history, color='blue', linewidth=2)\n",
    "            ax2.set_xlabel('Iteration')\n",
    "            ax2.set_ylabel('Loss (log scale)')\n",
    "            ax2.set_title('Training Loss (Log Scale)')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Initial Loss: {self.loss_history[0]:.6f}\")\n",
    "        print(f\"Final Loss: {self.loss_history[-1]:.6f}\")\n",
    "        print(f\"Loss Reduction: {((self.loss_history[0] - self.loss_history[-1]) / self.loss_history[0] * 100):.2f}%\")\n",
    "        print(f\"Total Iterations: {len(self.loss_history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(learning_rate=0.1, max_iter=1500, batch_size=1000)\n",
    "model.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = calculate_metrics(y_train, np.where(model.predict(X_train) == 1, 'spam', 'ham'), \"Training\")\n",
    "print(\"--------------------------------\")\n",
    "_ = calculate_metrics(y_val, np.where(model.predict(X_val) == 1, 'spam', 'ham'), \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thử nghiệm thực tế"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(subject='', message=''):\n",
    "    if pd.isnull(subject):\n",
    "        subject = ''\n",
    "    if pd.isnull(message):\n",
    "        message = ''\n",
    "    return subject + ' ' + message\n",
    "\n",
    "def predict_email(subject, message, vectorizer, model):\n",
    "    \"\"\"\n",
    "    Predict spam/ham for a single email.\n",
    "    Args:\n",
    "        subject (str): Email subject\n",
    "        message (str): Email body\n",
    "        vectorizer (CountVectorizer): Fitted vectorizer\n",
    "        model (LogisticRegression): Trained model\n",
    "    Returns:\n",
    "        int: 1 for spam, 0 for ham\n",
    "    \"\"\"\n",
    "    text = preprocess_text(subject, message)\n",
    "    X = vectorizer.transform([text])\n",
    "    pred = model.predict(X)[0]\n",
    "    if type(pred) == np.int32:\n",
    "        return \"spam\" if pred == 1 else \"ham\"\n",
    "    else:\n",
    "        return pred\n",
    "\n",
    "def predict_csv(csv_path, vectorizer, model, label_col='Spam/Ham'):\n",
    "    \"\"\"\n",
    "    Predict and evaluate on a CSV file with columns: Subject, Message, Spam/Ham.\n",
    "    Args:\n",
    "        csv_path (str): Path to CSV file\n",
    "        vectorizer (CountVectorizer): Fitted vectorizer\n",
    "        model (LogisticRegression): Trained model\n",
    "        label_col (str): Name of label column\n",
    "    Returns:\n",
    "        dict: Metrics (accuracy, precision, recall, f1_score, confusion_matrix)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['Subject'] = df['Subject'].fillna('')\n",
    "    df['Message'] = df['Message'].fillna('')\n",
    "    df['text'] = df['Subject'] + ' ' + df['Message']\n",
    "    X = vectorizer.transform(df['text'])\n",
    "    y_true = df[label_col].values\n",
    "    y_pred = model.predict(X)\n",
    "    # Convert numeric prediction to label if needed\n",
    "    if y_pred[0].dtype == np.int32:\n",
    "        y_pred_label = ['spam' if p == 1 else 'ham' for p in y_pred]\n",
    "    else:\n",
    "        y_pred_label = y_pred\n",
    "    # Simple metrics\n",
    "    y_pred_label = np.asarray(y_pred_label)\n",
    "    TP = np.sum((y_pred_label == 'spam') & (y_true == 'spam'))\n",
    "    FP = np.sum((y_pred_label == 'spam') & (y_true == 'ham'))\n",
    "    TN = np.sum((y_pred_label == 'ham') & (y_true == 'ham'))\n",
    "    FN = np.sum((y_pred_label == 'ham') & (y_true == 'spam'))\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thử nghiệm với 1 email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "lr_model = pickle.load(open('Models/lr_model.pkl', 'rb'))\n",
    "nb_model = pickle.load(open('Models/nb_model.pkl', 'rb'))\n",
    "bow = pickle.load(open('Models/BoW.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT = None\n",
    "MESSAGE = None\n",
    "\n",
    "if not SUBJECT and not MESSAGE:\n",
    "    SUBJECT = \"Hello\"\n",
    "    MESSAGE = \"This is a test message\"\n",
    "\n",
    "print(\"Predicting email with Naive Bayes Classifier: \", end=\"\")\n",
    "print(predict_email(SUBJECT, MESSAGE, bow, nb_model))\n",
    "print(\"--------------------------------\")\n",
    "print(\"Predicting email with Logistic Regression Classifier: \", end=\"\")\n",
    "print(predict_email(SUBJECT, MESSAGE, bow, lr_model))\n",
    "print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thử nghiệm với 1 file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "lr_model = pickle.load(open('Models/lr_model.pkl', 'rb'))\n",
    "nb_model = pickle.load(open('Models/nb_model.pkl', 'rb'))\n",
    "bow = pickle.load(open('Models/BoW.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = None\n",
    "\n",
    "if not CSV_PATH:\n",
    "    CSV_PATH = \"Data/val.csv\"\n",
    "\n",
    "print(\"Predicting email with Logistic Regression Classifier\")\n",
    "predict_csv(CSV_PATH, bow, lr_model)\n",
    "print(\"--------------------------------\")\n",
    "print(\"Predicting email with Naive Bayes Classifier\")\n",
    "predict_csv(CSV_PATH, bow, nb_model)\n",
    "print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
